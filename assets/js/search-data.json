{
  
    
        "post0": {
            "title": "Mathematics for Deep Learning - A Cookbook",
            "content": "This blog post serves as a quick review for the mathematics of my Deep Learning class. . Probability . Probability can crudely be of two kinds: . Frequency Probability | Bayesian probability | But with a small set of common sense assumptions, we treat bayesian probability to be exactly same as frequency probability. . Random Variables . Variables that can take different values randomly. They can be used as descriptors of the states that are possible and how likely they are. . They can be continuous or discrete. Set of states in denoted by $ bold{x}$ and individual values as ${x_1}, {x_2}$, etc. . Probability Mass Functions (PMF) . Probability Mass Function (PMF) maps the discrete state of the random variable to it’s probability. It’s usually denoted as $P( bold{x}$. . There are there conditions for a PMF must satisfy: . The domain of P must be the set of all possible states of $ bold{x}$ | $ forall x in bold{x}, 0 leq P(x) leq 1$ | $ forall {x} in bold{x}, sum P(x) = 1$ | Probability Density Functions (PDF) . When the states are continuous, Probability Density Functions maps real values of random variable to probability of states. It’s usually denoted by p. . Analogous to PMF, the conditions for a PDF are: . The domain of p must be set of all possible states of x | $ forall x in bold{x}, p(x) geq 0$ only | $ int p(x) , dx = 1$ | Common Operations on PMFs and PDFs . Marginal Probability . Remove one dimension of dependency by summing across it i.e. marginalize it. . $ forall x in bold{x}, P( bold{x} = x) = displaystyle sum_y P( bold{x} = x, bold{y} = y)$ . and . $p(x) = int p(x, y) , dy$ . Conditional Probability . Computes the probabilty of one event happening given that another has happened. . $P( bold{y} = y mid bold{x} = x) = frac{ displaystyle P( bold{y} = y, bold{x} = x)}{ displaystyle P( bold{x} = x)}$ . It can only be computed if $P( bold{x}) &gt; 0$. . Note: This is not the same as what would happen if some action was taken. Those are inversion queries in the domain of causal modeling. . The Chain Rule of Conditional Probability . Using the above, any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable. . $P( bold{x} ^{(1)},…, bold{x} ^{(n)}) = P( bold{x} ^{(1)}) prod limits_{i=2}^n P( bold{x} ^{(i)} mid bold{x} ^{(1)},…, bold{x} ^{(i-1)})$ . Independence and Conditional Independence . Random variable $x$ and $y$ are independent if their joint probability distribution can be expressed as a product of their individual probability distribution. . $p( bold{x} = x, bold{y} = y) = p( bold{x} = x) cdot p( bold{y} = y)$ . Conditional independence is independence, given another random variable $z$. . $p( bold{x} = x, bold{y} = y mid bold{z} = z) = p( bold{x} = x mid bold{z} = z) cdot p( bold{y} = y mid bold{z} = z)$ . Expectance, Variance and Covariance . The expectation of expected value of a function $f(x)$ is the mean or average value it takes when the random variable $x$ is drawn from the probability distribution $P(x)$ . $E_{x sim P(x)}[f(x)] = displaystyle sum_x P(x)f(x)$ . $E_{x sim p(x)}[f(x)] = displaystyle int p(x)f(x) , dx$ . Expectations are linear i.e. . $E[ alpha f(x) + beta g(x)] = alpha E[f(x)] + beta E[g(x)]$ . if $ alpha$ and $ beta$ are independent of $x$. . Variance is a measure of how much the value deviates from the expected values. . $Var(f(x)) = E[f(x) - E[f(x)]]$ . Square root of variance is the standard deviation. . The covariance gives some sense of how much two function values are linearly related to each other, as well as the scale of these values. . $Cov(f(x), g(x)) = E[(f(x) - E[f(x)])(g(x) - E[g(x)])]$ . High absolute value means the values change very much and are far from their respective means at the same time (their scale). | If positive, both functions tends to take high values simultaneously (linear relation). | If negative, one function tends to take higher values while the other takes lower and vise versa (linear relation). | . Correlation normalizes each variable in order to measure relation independent of scale. . Two functions on random variables can have zero covariance, and still be dependent. Eg: $f(x) = x, x in U[-1, 1]$ and $g(x) = sx$ where $s$ is a random variable with $1/2$ probability to be $1$ and rest to be $-1$. Although $g(x)$ is clearly dependent on $x$, $Cov(f(x), g(x)) = 0$. The reason it is not the sufficient condition for independence is it doesn’t account for non-linear relationship. | But if covariance is non-zero, the two functions have some linear dependency. | . Covariance for a random vector $ bold{x} in mathbb{R}^n$ is an $N times N$ matrix such that . $Cov(x)_{i, j} = Cov(x_i, x_j)$ . and diagonal elements of the covariance matrix give the variance: . $diag(Cov(x)) = Var(x)$ . Common Probability Distributions . Discrete Probability Distribution . Bernoulli and Multinoulli distributions are enough to describe any discrete distribution over their domain, since the domain is very simple. . Bernoulli Distribution . It’s a distribution over a single binary variable. It’s main parameter is $ phi in [0, 1]$. . $P(x = 1) = phi$ $P(x = 0) = 1 - phi$ $x in [0, 1], P( bold{x} = x) = phi ^ x (1 - phi) ^ {1 -x}$ $E[x] = phi$ $Var(x) = phi(1 - phi)$ . Multinoulli Distribution . It’s a distribution over a single discrete variable with k different states, where k is finite. It is parameterize by a vector $ bold{p} in [0, 1] ^ k -1$, where $p_i$ is the probability of $ith$ state. The $kth$ state probability is $1 - bold{1}^T bold{p}$. Therefore, $ bold{1}^T bold{p} leq 1$. Since the probabilities aren’t defined, expectation and variance are not calculated for this distribution. . Continuous Probability Distributions . When dealing with continuous distributions, there are infinite states, so any distribution with small number of parameters must impose strict limits on the distribution. . Gaussian Distribution . The most common distribution over real numbers is the gaussian distribution. . $ mathcal{N}(x; mu, sigma^2) = sqrt{ displaystyle frac{1}{2 pi sigma^2}} Bigg( displaystyle frac{-1}{2 sigma^2} (x - mu)^2 Bigg)$ . Where $ mu$ is the mean and $ sigma^2$ is the standard deviation for the distribution. An even better way to parameterize the normal distribution is using precision or inverse variance $ beta$: . $ mathcal{N}(x; mu, beta^-1) = sqrt{ displaystyle frac{ beta}{2 pi}} Bigg( displaystyle frac{- beta}{2} (x - mu)^2 Bigg)$ . Standard normal distribution is with $ mu = 0$ and $ sigma = 1$ . In the absence of prior knowledge about distribution for a random variable, normal distribution is a good choice for two main reasons: . The central limit theorem shows that the sum of many independent random is approximately normal distribution. So many complicated systems can be modelled as normal distribution noise. | Out of all possible probability distributions with same variance, normal distribution encodes maximum about of uncertainity and least amount of prior. | Exponential and Laplace Distributions . Sometimes we need probability distributions that model a sharp point at $x = 0$. Exponential distribution is used in such cases: . $p(x; lambda) = lambda bold{1}_{x geq 0}exp(- lambda x)$ . Exponential distribution uses the condition bold{1}_{x geq 0} to zero out all values of distribution for negative values of $x$. . Another distribution that puts sharp peak at an arbitrary point $ mu$ is Laplace Distribution: . $Laplace(x; mu, gamma) = displaystyle frac{1}{2 gamma} exp Bigg(- frac{ | x - mu | }{ gamma} Bigg)$ | . The Dirac Distribution and Empirical Distribution . Mixtures of Distribution . Status: Work in Progress. .",
            "url": "https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html",
            "relUrl": "/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html",
            "date": " • Dec 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Process Synchronization",
            "content": "The Critical Section Problem . The critical section contain variables shared among different processes. The values of these may change depending on the order in which in is accessed by various processes. This can result in corruption of data. Thus, only one process should be allowed to enter the critical section at a time. . The solutions to this critical section problem need to satisfy the following three conditions: . Mutual Exclusion: Only one process enters the critical section at a time. | Progress: If no processes are executing in the critical section, and there are other processes waiting, the selection of the next process cannot be postponed indefinitely. | Bounded Waiting: There must be a bound on the number of processes that are allowed to go in the critical section (maybe because of higher priority) before a process that has requested to enter the critical section. This is to ensure there is no starvation. | In this post, we talk about methods used in synchronization of threads utilizing different cores in the same CPU. Process synchronization in distributed system uses the same underlying principles, but is a little different. That’s for another post. . Hardware Solutions . These involves changing making changes to the hardware environment the threads are running in. After this eases the burden of the programmer to some extent. . 1. TestAndSet . Although not truly a hardware solution, TestAndSet keeps track of a lock variable. Before entering the critical section, the process checks if the section is locked. If not locked, it proceeds, else, it waits. . As you can imagine, this satisfies mutual exclusion and progress, but isn’t bounded. The resources can be locked for a long long time. If the are unlocked preemptively, there’s no saying if the data will be corrupted. . 2. Disable Interrupts . In uni-processor systems, A naive approach is to just disable interrupts. Without interrupts, two processes will never enter the critical section. Since the processes are executed in a non-preemptive environment now, it can lead to a host of problems like Convoy Effect, starvation, etc. . Software Solutions . In these solutions the programmer handles which process enters the critical section and how the process leaves it. . 1. Turn Method . Just have a turn variable that specifies which process should enter the critical section, looping over all processes. . // code of process 0 while (true) { while (turn != 0); [CRITICAL SECTION] turn = 1; [EXIT SECTION] } . // code of process 1 while (true) { while (turn != 1); [CRITICAL SECTION] turn = 0; [EXIT SECTION] } . This ensures mutual exclusion, but not progress, as there is no rule that takes in account waiting processes to be selected immediately. . How about a boolean to know which process wants to enter the critical section? . // code of process 0 while (true) { wantsToEnter[0] = true; while (wantsToEnter[1] != 0); [CRITICAL SECTION] wantesToEnter[0] = false; [EXIT SECTION] } . // code of process 1 while (true) { wantsToEnter[1] = true; while (wantsToEnter[0]); [CRITICAL SECTION] wantesToEnter[1] = false; [EXIT SECTION] } . Limitations of Turn Method . This is both mutually exclusive and ensures progress. But if the processes context switch at the same time, both the process will be marked interested in entering. This will cause a deadlock. . 2. Peterson’s Algorithm . Peterson’s solution combines the above two approaches. There are two shared variables: . bool interestedInEntering[N] where N is the total number of processes. | int turn where turn is the process selected to enter. | Here’s the pseudo-code: . // code of process 0 while (true) { wantsToEnter[0] = true; turn = 1; while (turn == 1 &amp;&amp; wantsToEnter[1]); [CRITICAL SECTION] turn = 0; wantsToEnter[0] = false; [EXIT SECTION] } . // code of process 1 while (true) { wantsToEnter[1] = true; turn = 0; while (turn == 0 &amp;&amp; wantsToEnter[0]); [CRITICAL SECTION] wantsToEnter[1] = false; [EXIT SECTION] } . Limitations of Peterson’s Algorithm . This solution satisfies all three above mentioned properties, but it can’t scale to more than two processes since branching will need to be added for each process. . 3. Semaphores . Semaphores was proposed by Dijkstra in 1965. It is a variable shared between threads governing which process will enter the critical condition. Semaphores can be of two types. These are implemented primarily using two atomic functions. A sleep/wait function, and a wake-up/signal function. The wait functions makes new processes wait, and push them to a process queue. Signal function process dequeue waiting processes, signaling them. . An operation in called atomic if it executes as a single step relative to other threads. This is easier said than done (for example, a single 64-bit assignment when compiled using 32-bit executes as two instructions.) . Binary Semaphore (Mutex lock) - It’s a lock that we put on the shared resource before using it, and release the lock after using it. No other thread can access the resource when the lock is set. . | Counting Semaphores - These locks can take a range of values and as you an imagine, are used to put lock on resources with multiple instances. . | struct semaphore { int value; // n is the number of instances of the resource // q contains all Process Control Blocks (PCBs) // corresponding to processes got blocked // while performing down operation. Queue&lt;process&gt; q; } void Wait(semaphore s) { s.value = s.value - 1; if (s.value &lt; 0) { // here p is a process which is currently executing q.push(p); block(); } } void Signal(semaphore s) { s.value = s.value + 1; if (s.value &lt;= 0) { // remove process p from queue Process p=q.pop(); wakeup(p); } } struct semaphore s; while (true) { Wait(s); [CRITICAL SECTION] Signal(s) [EXIT SECTION] } . Limitations of Semaphore . Priority inversion: This should be handled with passing the processes in a priority queue, but that leads to unbounded waiting and in turn doesn’t satisfy the critical section problem. | Deadlock: Deadlock can occur when a process, after finishing it’s critical section execution, is trying to wake up a another process which isn’t asleep. | Busy waiting: The condition while (s.value == 0); is pooled repeatedly. The process is not doing anything useful in this time. | Note: This information can be found of other CS portals. This is a half-assed post I intend to come back to later with some insights. .",
            "url": "https://abhichou4.github.io/blog/operating%20systems/2021/08/06/Process-Synchronization-In-DS.html",
            "relUrl": "/operating%20systems/2021/08/06/Process-Synchronization-In-DS.html",
            "date": " • Aug 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Abhineet. I’m a final year Computer Science student. . . I like to use NoobMaster as my nom-de-net. Apart from the deceleration “I’m no expert at this”, it reminds me how you can study something for long and know so little about it. That’s precisely the objective of this blog. I’ll visited or re-visit topics from the perspective of a noob, and write about it for my own learning process, but if it helps someone out there, great! . You can find my resume on my linkedin. .",
          "url": "https://abhichou4.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://abhichou4.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}