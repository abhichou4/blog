<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://abhichou4.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://abhichou4.github.io/blog/" rel="alternate" type="text/html" /><updated>2021-12-06T07:49:18-06:00</updated><id>https://abhichou4.github.io/blog/feed.xml</id><title type="html">Abhineet’s Blog</title><subtitle>NoobMaster</subtitle><entry><title type="html">Mathematics for Deep Learning - A Cookbook</title><link href="https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html" rel="alternate" type="text/html" title="Mathematics for Deep Learning - A Cookbook" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook</id><content type="html" xml:base="https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html">&lt;p&gt;This blog post serves as a quick review for the mathematics of my Deep Learning class.&lt;/p&gt;

&lt;h2 id=&quot;probability&quot;&gt;Probability&lt;/h2&gt;

&lt;p&gt;Probability can crudely be of two kinds:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Frequency Probability&lt;/li&gt;
  &lt;li&gt;Bayesian probability&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But with a small set of common sense assumptions, we treat bayesian probability to be exactly same as frequency probability.&lt;/p&gt;

&lt;h3 id=&quot;random-variables&quot;&gt;Random Variables&lt;/h3&gt;

&lt;p&gt;Variables that can take different values randomly. They can be used as descriptors of the states that are possible and how likely they are.&lt;/p&gt;

&lt;p&gt;They can be continuous or discrete. Set of states in denoted by $\bold{x}$ and individual values as ${x_1}, {x_2}$, etc.&lt;/p&gt;

&lt;h3 id=&quot;probability-mass-functions-pmf&quot;&gt;Probability Mass Functions (PMF)&lt;/h3&gt;

&lt;p&gt;Probability Mass Function (PMF) maps the discrete state of the random variable to it’s probability. It’s usually denoted as $P(\bold{x}$.&lt;/p&gt;

&lt;p&gt;There are there conditions for a PMF must satisfy:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The domain of P must be the set of all possible states of $\bold{x}$&lt;/li&gt;
  &lt;li&gt;$\forall x \in \bold{x}, 0 \leq P(x) \leq 1$&lt;/li&gt;
  &lt;li&gt;$\forall {x} \in \bold{x}, \sum P(x) = 1$&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;probability-density-functions-pdf&quot;&gt;Probability Density Functions (PDF)&lt;/h3&gt;

&lt;p&gt;When the states are continuous, Probability Density Functions maps real values of random variable to probability of states. It’s usually denoted by p.&lt;/p&gt;

&lt;p&gt;Analogous to PMF, the conditions for a PDF are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The domain of p must be set of all possible states of x&lt;/li&gt;
  &lt;li&gt;$\forall x \in \bold{x}, p(x) \geq 0$ only&lt;/li&gt;
  &lt;li&gt;$\int p(x) \, dx = 1$&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;common-operations-on-pmfs-and-pdfs&quot;&gt;Common Operations on PMFs and PDFs&lt;/h3&gt;

&lt;h4 id=&quot;marginal-probability&quot;&gt;Marginal Probability&lt;/h4&gt;

&lt;p&gt;Remove one dimension of dependency by summing across it i.e. marginalize it.&lt;/p&gt;

&lt;p&gt;$\forall x \in \bold{x}, P(\bold{x} = x) = \displaystyle\sum_y P(\bold{x} = x, \bold{y} = y)$&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;$p(x) = \int p(x, y) \, dy$&lt;/p&gt;

&lt;h4 id=&quot;conditional-probability&quot;&gt;Conditional Probability&lt;/h4&gt;

&lt;p&gt;Computes the probabilty of one event happening given that another has happened.&lt;/p&gt;

&lt;p&gt;$P(\bold{y} = y \mid \bold{x} = x) = \frac{\displaystyle P(\bold{y} = y, \bold{x} = x)}{\displaystyle P(\bold{x} = x)}$&lt;/p&gt;

&lt;p&gt;It can only be computed if $P(\bold{x}) &amp;gt; 0$.&lt;/p&gt;

&lt;p&gt;Note: This is not the same as what would happen if some action was taken. Those are inversion queries in the domain of causal modeling.&lt;/p&gt;

&lt;h3 id=&quot;the-chain-rule-of-conditional-probability&quot;&gt;The Chain Rule of Conditional Probability&lt;/h3&gt;

&lt;p&gt;Using the above, any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable.&lt;/p&gt;

&lt;p&gt;$P(\bold{x} ^{(1)},…, \bold{x} ^{(n)}) = P(\bold{x} ^{(1)}) \prod\limits_{i=2}^n P(\bold{x} ^{(i)} \mid \bold{x} ^{(1)},…, \bold{x} ^{(i-1)})$&lt;/p&gt;

&lt;h3 id=&quot;independence-and-conditional-independence&quot;&gt;Independence and Conditional Independence&lt;/h3&gt;

&lt;p&gt;Random variable $x$ and $y$ are independent if their joint probability distribution can be expressed as a product of their individual probability distribution.&lt;/p&gt;

&lt;p&gt;$p(\bold{x} = x, \bold{y} = y) = p(\bold{x} = x) \cdot p(\bold{y} = y)$&lt;/p&gt;

&lt;p&gt;Conditional independence is independence, given another random variable $z$.&lt;/p&gt;

&lt;p&gt;$p(\bold{x} = x, \bold{y} = y \mid \bold{z} = z) = p(\bold{x} = x \mid \bold{z} = z) \cdot p(\bold{y} = y \mid \bold{z} = z)$&lt;/p&gt;

&lt;h3 id=&quot;expectance-variance-and-covariance&quot;&gt;Expectance, Variance and Covariance&lt;/h3&gt;

&lt;p&gt;The expectation of expected value of a function $f(x)$ is the mean or average value it takes when the random variable $x$ is drawn from the probability distribution $P(x)$&lt;/p&gt;

&lt;p&gt;$E_{x \sim P(x)}[f(x)] = \displaystyle\sum_x P(x)f(x)$&lt;/p&gt;

&lt;p&gt;$E_{x \sim p(x)}[f(x)] = \displaystyle\int p(x)f(x) \, dx$&lt;/p&gt;

&lt;p&gt;Expectations are linear i.e.&lt;/p&gt;

&lt;p&gt;$E[\alpha f(x) + \beta g(x)] = \alpha E[f(x)] + \beta E[g(x)]$&lt;/p&gt;

&lt;p&gt;if $\alpha$ and $\beta$ are independent of $x$.&lt;/p&gt;

&lt;p&gt;Variance is a measure of how much the value deviates from the expected values.&lt;/p&gt;

&lt;p&gt;$Var(f(x)) = E[f(x) - E[f(x)]]$&lt;/p&gt;

&lt;p&gt;Square root of variance is the standard deviation.&lt;/p&gt;

&lt;p&gt;The covariance gives some sense of how much two function values are linearly related to each other, as well as the scale of these values.&lt;/p&gt;

&lt;p&gt;$Cov(f(x), g(x)) = E[(f(x) - E[f(x)])(g(x) - E[g(x)])]$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High absolute value means the values change very much and are far from their respective means at the same time (their scale).&lt;/li&gt;
  &lt;li&gt;If positive, both functions tends to take high values simultaneously (linear relation).&lt;/li&gt;
  &lt;li&gt;If negative, one function tends to take higher values while the other takes lower and vise versa (linear relation).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Correlation normalizes each variable in order to measure relation independent of scale.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Two functions on random variables can have zero covariance, and still be dependent. Eg: $f(x) = x, x \in U[-1, 1]$ and $g(x) = sx$ where $s$ is a random variable with $1/2$ probability to be $1$ and rest to be $-1$. Although $g(x)$ is clearly dependent on $x$, $Cov(f(x), g(x)) = 0$. The reason it is not the sufficient condition for independence is it doesn’t account for non-linear relationship.&lt;/li&gt;
  &lt;li&gt;But if covariance is non-zero, the two functions have some linear dependency.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Covariance for a random vector $\bold{x} \in \mathbb{R}^n$ is an $N \times N$ matrix such that&lt;/p&gt;

&lt;p&gt;$Cov(x)_{i, j} = Cov(x_i, x_j)$&lt;/p&gt;

&lt;p&gt;and diagonal elements of the covariance matrix give the variance:&lt;/p&gt;

&lt;p&gt;$\sum diag(Cov(x)) = Var(x)$&lt;/p&gt;

&lt;h3 id=&quot;common-probability-distributions&quot;&gt;Common Probability Distributions&lt;/h3&gt;

&lt;h4 id=&quot;discrete-probability-distribution&quot;&gt;Discrete Probability Distribution&lt;/h4&gt;

&lt;p&gt;Bernoulli and Multinoulli distributions are enough to describe any discrete distribution over their domain, since the domain is very simple.&lt;/p&gt;

&lt;h5 id=&quot;bernoulli-distribution&quot;&gt;Bernoulli Distribution&lt;/h5&gt;

&lt;p&gt;It’s a distribution over a single binary variable. It’s main parameter is $\phi \in [0, 1]$.&lt;/p&gt;

&lt;p&gt;$P(x = 1) = \phi$ &lt;br /&gt;
$P(x = 0) = 1 - \phi$ &lt;br /&gt;
$x \in [0, 1], P(\bold{x} = x) = \phi ^ x (1 - \phi) ^ {1 -x}$ &lt;br /&gt;
$E[x] = \phi$ &lt;br /&gt;
$Var(x) = \phi(1 - \phi)$&lt;/p&gt;

&lt;h5 id=&quot;multinoulli-distribution&quot;&gt;Multinoulli Distribution&lt;/h5&gt;

&lt;p&gt;It’s a distribution over a single discrete variable with k different states, where k is finite. It is parameterize by a vector $\bold{p} \in [0, 1] ^ k -1$, where $p_i$ is the probability of $ith$ state. The $kth$ state probability is $1 - \bold{1}^T \bold{p}$. Therefore, $\bold{1}^T \bold{p} \leq 1$. Since the probabilities aren’t defined, expectation and variance are not calculated for this distribution.&lt;/p&gt;

&lt;h4 id=&quot;continuous-probability-distributions&quot;&gt;Continuous Probability Distributions&lt;/h4&gt;

&lt;p&gt;When dealing with continuous distributions, there are infinite states, so any distribution with small number of parameters must impose strict limits on the distribution.&lt;/p&gt;

&lt;h5 id=&quot;gaussian-distribution&quot;&gt;Gaussian Distribution&lt;/h5&gt;

&lt;p&gt;The most common distribution over real numbers is the gaussian distribution.&lt;/p&gt;

&lt;p&gt;$\mathcal{N}(x; \mu, \sigma^2) = \sqrt{\displaystyle\frac{1}{2 \pi \sigma^2}} \Bigg(\displaystyle\frac{-1}{2 \sigma^2} (x - \mu)^2\Bigg)$&lt;/p&gt;

&lt;p&gt;Where $\mu$ is the mean and $\sigma^2$ is the standard deviation for the distribution.&lt;/p&gt;

&lt;p&gt;An even better way to parameterize the normal distribution is using precision or inverse variance $\beta$:&lt;/p&gt;

&lt;p&gt;$\mathcal{N}(x; \mu, \beta^-1) = \sqrt{\displaystyle\frac{\beta}{2 \pi}} \Bigg(\displaystyle\frac{-\beta}{2} (x - \mu)^2\Bigg)$&lt;/p&gt;

&lt;p&gt;Standard normal distribution is with $\mu = 0$ and $\sigma = 1$&lt;/p&gt;

&lt;p&gt;In the absence of prior knowledge about distribution for a random variable, normal distribution is a good choice for two main reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The central limit theorem shows that the sum of many independent random is approximately normal distribution. So many complicated systems can be modelled as normal distribution noise.&lt;/li&gt;
  &lt;li&gt;Out of all possible probability distributions with same variance, normal distribution encodes maximum about of uncertainity and least amount of prior.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lastly, &lt;strong&gt;multivariant normal distribution&lt;/strong&gt; is:&lt;/p&gt;

&lt;p&gt;$\mathcal{N}(x; \mu, \sum) = \sqrt{\displaystyle\frac{1}{(2\pi)^n det(\sum)}} exp\Bigg(-\frac{1}{2}(x - \mu)^T \sum^{-1} (x - \mu)\Bigg)$&lt;/p&gt;

&lt;p&gt;and to avoid computing inverse, using the parameter &lt;strong&gt;precision matrix $\beta$&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$\mathcal{N}(x; \mu, \sum) = \sqrt{\displaystyle\frac{det(\beta)}{(2\pi)^n}} exp\Bigg(-\frac{1}{2}(x - \mu)^T \beta (x - \mu)\Bigg)$&lt;/p&gt;

&lt;h5 id=&quot;exponential-and-laplace-distributions&quot;&gt;Exponential and Laplace Distributions&lt;/h5&gt;

&lt;p&gt;Sometimes we need probability distributions that model a sharp point at $x = 0$. Exponential distribution is used in such cases:&lt;/p&gt;

&lt;p&gt;$p(x; \lambda) = \lambda \bold{1}_{x \geq 0}exp(-\lambda x)$&lt;/p&gt;

&lt;p&gt;Exponential distribution uses the condition \bold{1}_{x \geq 0} to zero out all values of distribution for negative values of $x$.&lt;/p&gt;

&lt;p&gt;Another distribution that puts sharp peak at an arbitrary point $\mu$ is Laplace Distribution:&lt;/p&gt;

&lt;p&gt;$Laplace(x; \mu, \gamma) = \displaystyle \frac{1}{2 \gamma} exp\Bigg(-\frac{\mid x - \mu \mid}{\gamma}\Bigg)$&lt;/p&gt;

&lt;h5 id=&quot;the-dirac-distribution-and-empirical-distribution&quot;&gt;The Dirac Distribution and Empirical Distribution&lt;/h5&gt;

&lt;p&gt;Dirac delta function is not a normal function that maps values of input to output but a generalized function that is defined in terms of its properties when integrated. It’s a limiting case of a series of functions that put less and less mass on all points other than zero, giving us an infinitely narrow and high peak at $x=0$.&lt;/p&gt;

&lt;p&gt;This can be used to model distributions that are clustered around some point $\mu$ as:&lt;/p&gt;

&lt;p&gt;$p(x; \mu) = \delta(x - \mu)$&lt;/p&gt;

&lt;p&gt;Another common use of Dirac delta function is as a component of the empirical distribution:&lt;/p&gt;

&lt;p&gt;$p(\bold{x}; \mu) = \frac{1}{m} \displaystyle\sum_{i=1}^m \delta(\bold{x} - \bold{x}^{(i)})$&lt;/p&gt;

&lt;p&gt;Empirical Distribution can be viewed in two ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As a distribution sampled from a dataset of training exaples.&lt;/li&gt;
  &lt;li&gt;As probability density that maximizes the likelihood of training data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;mixtures-of-distribution&quot;&gt;Mixtures of Distribution&lt;/h4&gt;

&lt;p&gt;Mixture distribution is used to create richer distributions with combining multiple distributions.&lt;/p&gt;

&lt;p&gt;On each trial, the choice of which component distribution generates the sample is identified using a multinoulli distribution:&lt;/p&gt;

&lt;p&gt;$P(x) = \displaystyle\sum_{i}P(c = i)P(x \mid c = i)$&lt;/p&gt;

&lt;p&gt;where $P(c)$ is multinoulli distribution over component identities.&lt;/p&gt;

&lt;p&gt;Empirical Distribution is one example of a continuous mixture distribution: one Dirac delta component for each training example.&lt;/p&gt;

&lt;p&gt;Latent variable is a random variable that is not observed directly. The latent variable may be related to a random variable $x$ through a joint distribution. In case of mixture models, its $P(x, c) = P(c)P(x \mid c)$.&lt;/p&gt;

&lt;p&gt;Another powerful mixture distribution is gaussian mixture distribution where $p(x \mid c = i)$ is a Gaussian. Each component has a mean $\mu^{(i)}$ and covariance $\bold{\sum}^{(i)}$.&lt;/p&gt;

&lt;p&gt;Just like in $P(x \mid c)$ $c$ is seen prior to seeing $x$, P(c \mid x)$ is &lt;strong&gt;posterior probability&lt;/strong&gt;, because it is computed after computing $x$.&lt;/p&gt;

&lt;p&gt;A Gaussian mixture model is a universal approximator i.e. any smooth distribution can be modelled with enough number of Gaussian mixture model with a non-zero error.&lt;/p&gt;

&lt;h3 id=&quot;bayes-rule&quot;&gt;Bayes Rule&lt;/h3&gt;

&lt;p&gt;$P(x \mid y) = \displaystyle\frac{P(y \mid x) P(x)}{P(y)}$&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;$P(y) = \sum_x P(y \mid x) P(x)$&lt;/p&gt;

&lt;h3 id=&quot;some-common-function-and-their-properties&quot;&gt;Some common function and their properties&lt;/h3&gt;

&lt;p&gt;These are some common functions that come up in the context of deep learning and some of their properties.&lt;/p&gt;

&lt;h4 id=&quot;logistic-sigmoid&quot;&gt;logistic sigmoid&lt;/h4&gt;

&lt;p&gt;$\sigma(x)) = \displaystyle\frac{1}{1 + exp(-x)}$&lt;/p&gt;

&lt;p&gt;Often used to compute $\phi \in (0, 1)$ in Bernoulli distributions.&lt;/p&gt;

&lt;h4 id=&quot;softplus&quot;&gt;Softplus&lt;/h4&gt;

&lt;p&gt;$\zeta(x) = log(1 + exp(x))$&lt;/p&gt;

&lt;p&gt;It is a smoother version of $max{0, x}$. Often used to compute $\sum$ and $\beta \in (0, \infty)$ for normal distributions.&lt;/p&gt;

&lt;h4 id=&quot;properties&quot;&gt;Properties&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$\displaystyle\frac{d\sigma(x)}{dx} = \sigma(x) (1 - \sigma(x))$&lt;/li&gt;
  &lt;li&gt;$1 - \sigma(x) = \sigma(-x)$&lt;/li&gt;
  &lt;li&gt;$log \sigma(x) = \zeta(-x)$&lt;/li&gt;
  &lt;li&gt;$\displaystyle\frac{d\zeta(x)}{dx} = \sigma(x)$&lt;/li&gt;
  &lt;li&gt;$\forall x \in (0, 1), \sigma^-1(x) = log(\displaystyle\frac{x}{1 - x}$&lt;/li&gt;
  &lt;li&gt;$\forall x &amp;gt; 0, \zeta^-1(x) = log(exp(x) - 1)$&lt;/li&gt;
  &lt;li&gt;$\zeta(x) = \int_{-\infty}^{x}\sigma(y)dy$&lt;/li&gt;
  &lt;li&gt;$\zeta(x) - \zeta(-x) = x$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The last property further reinforces $\zeta(x)$ is analogous to $x^+ = max{0, x}$ as $x^+ - x^- = x$ too.&lt;/p&gt;

&lt;h4 id=&quot;note-about-continuous-variables&quot;&gt;Note about continuous variables&lt;/h4&gt;

&lt;h2 id=&quot;information-theory&quot;&gt;Information Theory&lt;/h2&gt;

&lt;p&gt;Status: Work in Progress.&lt;/p&gt;</content><author><name></name></author><category term="Mathematics" /><category term="Deep Learning" /><summary type="html">This blog post serves as a quick review for the mathematics of my Deep Learning class.</summary></entry><entry><title type="html">Process Synchronization</title><link href="https://abhichou4.github.io/blog/operating%20systems/2021/08/06/Process-Synchronization-In-DS.html" rel="alternate" type="text/html" title="Process Synchronization" /><published>2021-08-06T00:00:00-05:00</published><updated>2021-08-06T00:00:00-05:00</updated><id>https://abhichou4.github.io/blog/operating%20systems/2021/08/06/Process-Synchronization-In-DS</id><content type="html" xml:base="https://abhichou4.github.io/blog/operating%20systems/2021/08/06/Process-Synchronization-In-DS.html">&lt;h3 id=&quot;the-critical-section-problem&quot;&gt;The Critical Section Problem&lt;/h3&gt;

&lt;p&gt;The critical section contain variables shared among different processes. The values of these may change
depending on the order in which in is accessed by various processes. This can result in corruption of data. Thus, only one process should
be allowed to enter the critical section at a time.&lt;/p&gt;

&lt;p&gt;The solutions to this critical section problem need to satisfy the following three conditions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mutual Exclusion: Only one process enters the critical section at a time.&lt;/li&gt;
  &lt;li&gt;Progress: If no processes are executing in the critical section, and there are other processes waiting, the selection of the next process
cannot be postponed indefinitely.&lt;/li&gt;
  &lt;li&gt;Bounded Waiting: There must be a bound on the number of processes that are allowed to go in the critical section (maybe because of higher priority) before a process that has requested to enter the critical section. This is to ensure there is no starvation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post, we talk about methods used in synchronization of threads utilizing different cores in the same CPU. Process synchronization in distributed system uses the same underlying principles, but is a little different. That’s for another post.&lt;/p&gt;

&lt;h3 id=&quot;hardware-solutions&quot;&gt;Hardware Solutions&lt;/h3&gt;

&lt;p&gt;These involves changing making changes to the hardware environment the threads are running in. After this eases the burden of the programmer to some extent.&lt;/p&gt;

&lt;h4 id=&quot;1-testandset&quot;&gt;1. TestAndSet&lt;/h4&gt;

&lt;p&gt;Although not truly a hardware solution, TestAndSet keeps track of a lock variable. Before entering the critical section, the process checks if the
section is locked. If not locked, it proceeds, else, it waits.&lt;/p&gt;

&lt;p&gt;As you can imagine, this satisfies mutual exclusion and progress, but isn’t bounded. The resources can be locked for a long long time. If the are
unlocked preemptively, there’s no saying if the data will be corrupted.&lt;/p&gt;

&lt;h4 id=&quot;2-disable-interrupts&quot;&gt;2. Disable Interrupts&lt;/h4&gt;

&lt;p&gt;In uni-processor systems, A naive approach is to just disable interrupts. Without interrupts, two processes will never enter the critical section. Since the processes are executed in a non-preemptive environment now, it can lead to a host of problems like Convoy Effect, starvation, etc.&lt;/p&gt;

&lt;h3 id=&quot;software-solutions&quot;&gt;Software Solutions&lt;/h3&gt;

&lt;p&gt;In these solutions the programmer handles which process enters the critical section and how the process leaves it.&lt;/p&gt;

&lt;h4 id=&quot;1-turn-method&quot;&gt;1. Turn Method&lt;/h4&gt;

&lt;p&gt;Just have a turn variable that specifies which process should enter the critical section, looping over all processes.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// code of process 0
while (true) {
    while (turn != 0);
    [CRITICAL SECTION]
    turn = 1;
    [EXIT SECTION]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// code of process 1
while (true) {
    while (turn != 1);
    [CRITICAL SECTION]
    turn = 0;
    [EXIT SECTION]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This ensures mutual exclusion, but not progress, as there is no rule that takes in account waiting processes to be selected immediately.&lt;/p&gt;

&lt;p&gt;How about a boolean to know which process wants to enter the critical section?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// code of process 0
while (true) {
    wantsToEnter[0] = true;
    while (wantsToEnter[1] != 0);
    [CRITICAL SECTION]
    wantesToEnter[0] = false;
    [EXIT SECTION]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// code of process 1
while (true) {
    wantsToEnter[1] = true;
    while (wantsToEnter[0]);
    [CRITICAL SECTION]
    wantesToEnter[1] = false;
    [EXIT SECTION]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;limitations-of-turn-method&quot;&gt;Limitations of Turn Method&lt;/h5&gt;

&lt;p&gt;This is both mutually exclusive and ensures progress. But if the processes context switch at the same time, 
both the process will be marked interested in entering. This will cause a deadlock.&lt;/p&gt;

&lt;h4 id=&quot;2-petersons-algorithm&quot;&gt;2. Peterson’s Algorithm&lt;/h4&gt;

&lt;p&gt;Peterson’s solution combines the above two approaches. There are two shared variables:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bool interestedInEntering[N]&lt;/code&gt; where N is the total number of processes.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int turn&lt;/code&gt; where turn is the process selected to enter.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here’s the pseudo-code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// code of process 0
while (true) {
    wantsToEnter[0] = true;
    turn = 1;
    while (turn == 1 &amp;amp;&amp;amp; wantsToEnter[1]);
    [CRITICAL SECTION]
    turn = 0;
    wantsToEnter[0] = false;
    [EXIT SECTION]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// code of process 1
while (true) {
    wantsToEnter[1] = true;
    turn = 0;
    while (turn == 0 &amp;amp;&amp;amp; wantsToEnter[0]);
    [CRITICAL SECTION]
    wantsToEnter[1] = false;
    [EXIT SECTION]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;limitations-of-petersons-algorithm&quot;&gt;Limitations of Peterson’s Algorithm&lt;/h5&gt;

&lt;p&gt;This solution satisfies all three above mentioned properties, but it can’t scale to more than two processes since branching will need to be added
for each process.&lt;/p&gt;

&lt;h4 id=&quot;3-semaphores&quot;&gt;3. Semaphores&lt;/h4&gt;

&lt;p&gt;Semaphores was proposed by Dijkstra in 1965. It is a variable shared between threads governing which process will enter the critical condition.
Semaphores can be of two types. These are implemented primarily using two atomic functions. A sleep/wait function, and a wake-up/signal function. The wait functions makes new processes wait, and push them to a process queue. Signal function process dequeue waiting processes, signaling them.&lt;/p&gt;

&lt;p&gt;An operation in called atomic if it executes as a single step relative to other threads. This is easier said than done (for example, a single 64-bit assignment when compiled using 32-bit executes as two instructions.)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Binary Semaphore (Mutex lock) - It’s a lock that we put on the shared resource before using it, and release the lock after using it. No other thread can access
the resource when the lock is set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Counting Semaphores - These locks can take a range of values and as you an imagine, are used to put lock on resources with multiple instances.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;struct semaphore {
    int value; // n is the number of instances of the resource
    // q contains all Process Control Blocks (PCBs)
    // corresponding to processes got blocked
    // while performing down operation.
    Queue&amp;lt;process&amp;gt; q;
 
}

void Wait(semaphore s) {
    s.value = s.value - 1;
    if (s.value &amp;lt; 0) {
        // here p is a process which is currently executing
        q.push(p);
        block();
    }
}

void Signal(semaphore s) {
    s.value = s.value + 1;
    if (s.value &amp;lt;= 0) {
        // remove process p from queue
        Process p=q.pop();
        wakeup(p);
    }
}

struct semaphore s;
while (true) {
    Wait(s);
    [CRITICAL SECTION]
    Signal(s)
    [EXIT SECTION]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;limitations-of-semaphore&quot;&gt;Limitations of Semaphore&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;Priority inversion: This should be handled with passing the processes in a priority queue, but that leads to unbounded waiting and in turn doesn’t satisfy the critical section
problem.&lt;/li&gt;
  &lt;li&gt;Deadlock: Deadlock can occur when a process, after finishing it’s critical section execution, is trying to wake up a another process which isn’t asleep.&lt;/li&gt;
  &lt;li&gt;Busy waiting: The condition &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;while (s.value == 0);&lt;/code&gt; is pooled repeatedly. The process is not doing anything useful in this time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: This information can be found of other CS portals. This is a half-assed post I intend to come back to later with some insights.&lt;/p&gt;</content><author><name></name></author><category term="Operating Systems" /><summary type="html">The Critical Section Problem</summary></entry></feed>