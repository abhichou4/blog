<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Mathematics for Deep Learning - A Cookbook | Abhineet’s Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Mathematics for Deep Learning - A Cookbook" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Linear Algebra, Probability and Information theory used in Deep Learning" />
<meta property="og:description" content="Linear Algebra, Probability and Information theory used in Deep Learning" />
<link rel="canonical" href="https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html" />
<meta property="og:url" content="https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html" />
<meta property="og:site_name" content="Abhineet’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-12-06T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Linear Algebra, Probability and Information theory used in Deep Learning","url":"https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://abhichou4.github.io/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html"},"headline":"Mathematics for Deep Learning - A Cookbook","dateModified":"2021-12-06T00:00:00-06:00","datePublished":"2021-12-06T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://abhichou4.github.io/blog/feed.xml" title="Abhineet's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Abhineet&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/projects/">Projects</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Mathematics for Deep Learning - A Cookbook</h1><p class="page-description">Linear Algebra, Probability and Information theory used in Deep Learning</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-12-06T00:00:00-06:00" itemprop="datePublished">
        Dec 6, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Mathematics">Mathematics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Deep Learning">Deep Learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#probability">Probability</a>
<ul>
<li class="toc-entry toc-h3"><a href="#random-variables">Random Variables</a></li>
<li class="toc-entry toc-h3"><a href="#probability-mass-functions-pmf">Probability Mass Functions (PMF)</a></li>
<li class="toc-entry toc-h3"><a href="#probability-density-functions-pdf">Probability Density Functions (PDF)</a></li>
<li class="toc-entry toc-h3"><a href="#common-operations-on-pmfs-and-pdfs">Common Operations on PMFs and PDFs</a>
<ul>
<li class="toc-entry toc-h4"><a href="#marginal-probability">Marginal Probability</a></li>
<li class="toc-entry toc-h4"><a href="#conditional-probability">Conditional Probability</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#the-chain-rule-of-conditional-probability">The Chain Rule of Conditional Probability</a></li>
<li class="toc-entry toc-h3"><a href="#independence-and-conditional-independence">Independence and Conditional Independence</a></li>
<li class="toc-entry toc-h3"><a href="#expectance-variance-and-covariance">Expectance, Variance and Covariance</a></li>
<li class="toc-entry toc-h3"><a href="#common-probability-distributions">Common Probability Distributions</a>
<ul>
<li class="toc-entry toc-h4"><a href="#discrete-probability-distribution">Discrete Probability Distribution</a>
<ul>
<li class="toc-entry toc-h5"><a href="#bernoulli-distribution">Bernoulli Distribution</a></li>
<li class="toc-entry toc-h5"><a href="#multinoulli-distribution">Multinoulli Distribution</a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#continuous-probability-distributions">Continuous Probability Distributions</a>
<ul>
<li class="toc-entry toc-h5"><a href="#gaussian-distribution">Gaussian Distribution</a></li>
<li class="toc-entry toc-h5"><a href="#exponential-and-laplace-distributions">Exponential and Laplace Distributions</a></li>
<li class="toc-entry toc-h5"><a href="#the-dirac-distribution-and-empirical-distribution">The Dirac Distribution and Empirical Distribution</a></li>
</ul>
</li>
<li class="toc-entry toc-h4"><a href="#mixtures-of-distribution">Mixtures of Distribution</a></li>
</ul>
</li>
</ul>
</li>
</ul><p>This blog post serves as a quick review for the mathematics of my Deep Learning class.</p>

<h2 id="probability">
<a class="anchor" href="#probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Probability</h2>

<p>Probability can crudely be of two kinds:</p>

<ol>
  <li>Frequency Probability</li>
  <li>Bayesian probability</li>
</ol>

<p>But with a small set of common sense assumptions, we treat bayesian probability to be exactly same as frequency probability.</p>

<h3 id="random-variables">
<a class="anchor" href="#random-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Random Variables</h3>

<p>Variables that can take different values randomly. They can be used as descriptors of the states that are possible and how likely they are.</p>

<p>They can be continuous or discrete. Set of states in denoted by $\bold{x}$ and individual values as ${x_1}, {x_2}$, etc.</p>

<h3 id="probability-mass-functions-pmf">
<a class="anchor" href="#probability-mass-functions-pmf" aria-hidden="true"><span class="octicon octicon-link"></span></a>Probability Mass Functions (PMF)</h3>

<p>Probability Mass Function (PMF) maps the discrete state of the random variable to it’s probability. It’s usually denoted as $P(\bold{x}$.</p>

<p>There are there conditions for a PMF must satisfy:</p>

<ol>
  <li>The domain of P must be the set of all possible states of $\bold{x}$</li>
  <li>$\forall x \in \bold{x}, 0 \leq P(x) \leq 1$</li>
  <li>$\forall {x} \in \bold{x}, \sum P(x) = 1$</li>
</ol>

<h3 id="probability-density-functions-pdf">
<a class="anchor" href="#probability-density-functions-pdf" aria-hidden="true"><span class="octicon octicon-link"></span></a>Probability Density Functions (PDF)</h3>

<p>When the states are continuous, Probability Density Functions maps real values of random variable to probability of states. It’s usually denoted by p.</p>

<p>Analogous to PMF, the conditions for a PDF are:</p>

<ol>
  <li>The domain of p must be set of all possible states of x</li>
  <li>$\forall x \in \bold{x}, p(x) \geq 0$ only</li>
  <li>$\int p(x) \, dx = 1$</li>
</ol>

<h3 id="common-operations-on-pmfs-and-pdfs">
<a class="anchor" href="#common-operations-on-pmfs-and-pdfs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Common Operations on PMFs and PDFs</h3>

<h4 id="marginal-probability">
<a class="anchor" href="#marginal-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Marginal Probability</h4>

<p>Remove one dimension of dependency by summing across it i.e. marginalize it.</p>

<p>$\forall x \in \bold{x}, P(\bold{x} = x) = \displaystyle\sum_y P(\bold{x} = x, \bold{y} = y)$</p>

<p>and</p>

<p>$p(x) = \int p(x, y) \, dy$</p>

<h4 id="conditional-probability">
<a class="anchor" href="#conditional-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conditional Probability</h4>

<p>Computes the probabilty of one event happening given that another has happened.</p>

<p>$P(\bold{y} = y \mid \bold{x} = x) = \frac{\displaystyle P(\bold{y} = y, \bold{x} = x)}{\displaystyle P(\bold{x} = x)}$</p>

<p>It can only be computed if $P(\bold{x}) &gt; 0$.</p>

<p>Note: This is not the same as what would happen if some action was taken. Those are inversion queries in the domain of causal modeling.</p>

<h3 id="the-chain-rule-of-conditional-probability">
<a class="anchor" href="#the-chain-rule-of-conditional-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Chain Rule of Conditional Probability</h3>

<p>Using the above, any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable.</p>

<p>$P(\bold{x} ^{(1)},…, \bold{x} ^{(n)}) = P(\bold{x} ^{(1)}) \prod\limits_{i=2}^n P(\bold{x} ^{(i)} \mid \bold{x} ^{(1)},…, \bold{x} ^{(i-1)})$</p>

<h3 id="independence-and-conditional-independence">
<a class="anchor" href="#independence-and-conditional-independence" aria-hidden="true"><span class="octicon octicon-link"></span></a>Independence and Conditional Independence</h3>

<p>Random variable $x$ and $y$ are independent if their joint probability distribution can be expressed as a product of their individual probability distribution.</p>

<p>$p(\bold{x} = x, \bold{y} = y) = p(\bold{x} = x) \cdot p(\bold{y} = y)$</p>

<p>Conditional independence is independence, given another random variable $z$.</p>

<p>$p(\bold{x} = x, \bold{y} = y \mid \bold{z} = z) = p(\bold{x} = x \mid \bold{z} = z) \cdot p(\bold{y} = y \mid \bold{z} = z)$</p>

<h3 id="expectance-variance-and-covariance">
<a class="anchor" href="#expectance-variance-and-covariance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Expectance, Variance and Covariance</h3>

<p>The expectation of expected value of a function $f(x)$ is the mean or average value it takes when the random variable $x$ is drawn from the probability distribution $P(x)$</p>

<p>$E_{x \sim P(x)}[f(x)] = \displaystyle\sum_x P(x)f(x)$</p>

<p>$E_{x \sim p(x)}[f(x)] = \displaystyle\int p(x)f(x) \, dx$</p>

<p>Expectations are linear i.e.</p>

<p>$E[\alpha f(x) + \beta g(x)] = \alpha E[f(x)] + \beta E[g(x)]$</p>

<p>if $\alpha$ and $\beta$ are independent of $x$.</p>

<p>Variance is a measure of how much the value deviates from the expected values.</p>

<p>$Var(f(x)) = E[f(x) - E[f(x)]]$</p>

<p>Square root of variance is the standard deviation.</p>

<p>The covariance gives some sense of how much two function values are linearly related to each other, as well as the scale of these values.</p>

<p>$Cov(f(x), g(x)) = E[(f(x) - E[f(x)])(g(x) - E[g(x)])]$</p>

<ul>
  <li>High absolute value means the values change very much and are far from their respective means at the same time (their scale).</li>
  <li>If positive, both functions tends to take high values simultaneously (linear relation).</li>
  <li>If negative, one function tends to take higher values while the other takes lower and vise versa (linear relation).</li>
</ul>

<p>Correlation normalizes each variable in order to measure relation independent of scale.</p>

<ul>
  <li>Two functions on random variables can have zero covariance, and still be dependent. Eg: $f(x) = x, x \in U[-1, 1]$ and $g(x) = sx$ where $s$ is a random variable with $1/2$ probability to be $1$ and rest to be $-1$. Although $g(x)$ is clearly dependent on $x$, $Cov(f(x), g(x)) = 0$. The reason it is not the sufficient condition for independence is it doesn’t account for non-linear relationship.</li>
  <li>But if covariance is non-zero, the two functions have some linear dependency.</li>
</ul>

<p>Covariance for a random vector $\bold{x} \in \mathbb{R}^n$ is an $N \times N$ matrix such that</p>

<p>$Cov(x)_{i, j} = Cov(x_i, x_j)$</p>

<p>and diagonal elements of the covariance matrix give the variance:</p>

<p>$\sum diag(Cov(x)) = Var(x)$</p>

<h3 id="common-probability-distributions">
<a class="anchor" href="#common-probability-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Common Probability Distributions</h3>

<h4 id="discrete-probability-distribution">
<a class="anchor" href="#discrete-probability-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discrete Probability Distribution</h4>

<p>Bernoulli and Multinoulli distributions are enough to describe any discrete distribution over their domain, since the domain is very simple.</p>

<h5 id="bernoulli-distribution">
<a class="anchor" href="#bernoulli-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bernoulli Distribution</h5>

<p>It’s a distribution over a single binary variable. It’s main parameter is $\phi \in [0, 1]$.</p>

<p>$P(x = 1) = \phi$ <br>
$P(x = 0) = 1 - \phi$ <br>
$x \in [0, 1], P(\bold{x} = x) = \phi ^ x (1 - \phi) ^ {1 -x}$ <br>
$E[x] = \phi$ <br>
$Var(x) = \phi(1 - \phi)$</p>

<h5 id="multinoulli-distribution">
<a class="anchor" href="#multinoulli-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multinoulli Distribution</h5>

<p>It’s a distribution over a single discrete variable with k different states, where k is finite. It is parameterize by a vector $\bold{p} \in [0, 1] ^ k -1$, where $p_i$ is the probability of $ith$ state. The $kth$ state probability is $1 - \bold{1}^T \bold{p}$. Therefore, $\bold{1}^T \bold{p} \leq 1$. Since the probabilities aren’t defined, expectation and variance are not calculated for this distribution.</p>

<h4 id="continuous-probability-distributions">
<a class="anchor" href="#continuous-probability-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Continuous Probability Distributions</h4>

<p>When dealing with continuous distributions, there are infinite states, so any distribution with small number of parameters must impose strict limits on the distribution.</p>

<h5 id="gaussian-distribution">
<a class="anchor" href="#gaussian-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gaussian Distribution</h5>

<p>The most common distribution over real numbers is the gaussian distribution.</p>

<p>$\mathcal{N}(x; \mu, \sigma^2) = \sqrt{\displaystyle\frac{1}{2 \pi \sigma^2}} \Bigg(\displaystyle\frac{-1}{2 \sigma^2} (x - \mu)^2\Bigg)$</p>

<p>Where $\mu$ is the mean and $\sigma^2$ is the standard deviation for the distribution.</p>

<p>An even better way to parameterize the normal distribution is using precision or inverse variance $\beta$:</p>

<p>$\mathcal{N}(x; \mu, \beta^-1) = \sqrt{\displaystyle\frac{\beta}{2 \pi}} \Bigg(\displaystyle\frac{-\beta}{2} (x - \mu)^2\Bigg)$</p>

<p>Standard normal distribution is with $\mu = 0$ and $\sigma = 1$</p>

<p>In the absence of prior knowledge about distribution for a random variable, normal distribution is a good choice for two main reasons:</p>

<ol>
  <li>The central limit theorem shows that the sum of many independent random is approximately normal distribution. So many complicated systems can be modelled as normal distribution noise.</li>
  <li>Out of all possible probability distributions with same variance, normal distribution encodes maximum about of uncertainity and least amount of prior.</li>
</ol>

<p>Lastly, <strong>multivariant normal distribution</strong> is:</p>

<p>$\mathcal{N}(x; \mu, \sum) = \sqrt{\displaystyle\frac{1}{(2\pi)^n det(\sum)}} exp\Bigg(-\frac{1}{2}(x - \mu)^T \sum^{-1} (x - \mu)\Bigg)$</p>

<p>and to avoid computing inverse, using the parameter <strong>precision matrix $\beta$</strong>:</p>

<p>$\mathcal{N}(x; \mu, \sum) = \sqrt{\displaystyle\frac{det(\beta)}{(2\pi)^n}} exp\Bigg(-\frac{1}{2}(x - \mu)^T \beta (x - \mu)\Bigg)$</p>

<h5 id="exponential-and-laplace-distributions">
<a class="anchor" href="#exponential-and-laplace-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exponential and Laplace Distributions</h5>

<p>Sometimes we need probability distributions that model a sharp point at $x = 0$. Exponential distribution is used in such cases:</p>

<p>$p(x; \lambda) = \lambda \bold{1}_{x \geq 0}exp(-\lambda x)$</p>

<p>Exponential distribution uses the condition \bold{1}_{x \geq 0} to zero out all values of distribution for negative values of $x$.</p>

<p>Another distribution that puts sharp peak at an arbitrary point $\mu$ is Laplace Distribution:</p>

<p>$Laplace(x; \mu, \gamma) = \displaystyle \frac{1}{2 \gamma} exp\Bigg(-\frac{\mid x - \mu \mid}{\gamma}\Bigg)$</p>

<h5 id="the-dirac-distribution-and-empirical-distribution">
<a class="anchor" href="#the-dirac-distribution-and-empirical-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Dirac Distribution and Empirical Distribution</h5>

<p>Dirac delta function is not a normal function that maps values of input to output but a generalized function that is defined in terms of its properties when integrated. It’s a limiting case of a series of functions that put less and less mass on all points other than zero, giving us an infinitely narrow and high peak at $x=0$.</p>

<p>This can be used to model distributions that are clustered around some point $\mu$ as:</p>

<p>$p(x; \mu) = \delta(x - \mu)$</p>

<p>Another common use of Dirac delta function is as a component of the empirical distribution:</p>

<p>$p(\bold{x}; \mu) = \frac{1}{m} \displaystyle\sum_{i=1}^m \delta(\bold{x} - \bold{x}^{(i)})$</p>

<p>Empirical Distribution can be viewed in two ways:</p>

<ol>
  <li>As a distribution sampled from a dataset of training exaples.</li>
  <li>As probability density that maximizes the likelihood of training data.</li>
</ol>

<h4 id="mixtures-of-distribution">
<a class="anchor" href="#mixtures-of-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mixtures of Distribution</h4>

<p>Mixture distribution is used to create richer distributions with combining multiple distributions.</p>

<p>On each trial, the choice of which component distribution generates the sample is identified using a multinoulli distribution:</p>

<p>$P(x) = \displaystyle\sum_{i}P(c = i)P(x \mid c = i)$</p>

<p>where $P(c)$ is multinoulli distribution over component identities.</p>

<p>Empirical Distribution is one example of a continuous mixture distribution: one Dirac delta component for each training example.</p>

<p>Latent variable is a random variable that is not observed directly. The latent variable may be related to a random variable $x$ through a joint distribution. In case of mixture models, its $P(x, c) = P(c)P(x \mid c)$.</p>

<table>
  <tbody>
    <tr>
      <td>Another powerful mixture distribution is gaussian mixture distribution where p(x</td>
      <td>c = i) is a Gaussian. Each component has a mean $\mu^{(i)}$ and covariance $\bold{\sum}^{(i)}$. Some gaussian mixtures may have additional constraints like $\bold{\sum}^{(i)} = \bold{\sum},  \forall i$ or the covariance matrix may be diagonal or isotropic.</td>
    </tr>
  </tbody>
</table>

<p>Just like in $P(x \mid c)$ $c$ is seen prior to seeing $x$, P(c \mid x)$ is <strong>posterior probability</strong>, because it is computed after computing $x$.</p>

<p>A Gaussian mixture model is a universal approximator i.e. any smooth distribution can be modelled with enough number of Gaussian mixture model with a non-zero error.</p>

<p>Status: Work in Progress.</p>

  </div><a class="u-url" href="/blog/mathematics/deep%20learning/2021/12/06/Mathematics-For-Deep-Learning-A-Cookbook.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>NoobMaster</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/abhichou4" title="abhichou4"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.instagram.com/abhineet99" title="abhineet99"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#instagram"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/abhineet-c-344967bb" title="abhineet-c-344967bb"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/abhichou4" title="abhichou4"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
